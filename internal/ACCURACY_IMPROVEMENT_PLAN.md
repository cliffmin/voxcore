# VoxCore + VoxCompose Accuracy Improvement Plan

**Goal:** Increase transcription accuracy while maintaining VoxCore's stateless architecture and VoxCompose's stateful learning capabilities.

**Date:** 2025-12-04
**Status:** DRAFT - Planning Phase

---

## Executive Summary

**Current State:**
- VoxCore uses **static INITIAL_PROMPT** for Whisper vocabulary hints
- VoxCompose learns corrections **after transcription** (post-processing only)
- **No feedback loop** from user edits to improve future transcriptions
- **No confidence tracking** or quality metrics
- **Missing app context** - can't adapt to different use cases

**Key Insight:** VoxCompose runs AFTER Whisper, so it cannot currently improve Whisper's accuracy. It only refines text post-transcription.

**Proposed Solution:** Create a **bidirectional integration** where:
1. **VoxCompose generates vocabulary hints** from learned corrections → feeds to VoxCore
2. **VoxCore captures richer metadata** including confidence, app context, audio quality
3. **User feedback loop** allows learning from actual corrections, not just LLM diffs
4. **VoxCore remains stateless** - reads vocabulary from VoxCompose-generated files
5. **VoxCompose becomes smarter** - learns from real user behavior

---

## Problem Analysis

### Current Accuracy Limitations

#### 1. Word Concatenation Errors
**Evidence:** Hardcoded fixes in VoxCompose for `pushto`, `committhis`, `github`, `json`
**Root Cause:** Whisper tokenization struggles with technical terms and word boundaries
**Impact:** Common technical phrases misrecognized, requiring post-correction

#### 2. Technical Vocabulary
**Evidence:** Systematic lowercasing of `GitHub`, `JSON`, `API`, technical acronyms
**Root Cause:** base.en model lacks domain-specific vocabulary
**Impact:** Professional terms need manual correction

#### 3. No Real-Time Feedback
**Evidence:** VoxCompose learns from LLM diffs, not user edits
**Root Cause:** No capture of post-paste user corrections
**Impact:** Can't measure true accuracy or learn from actual mistakes

#### 4. Static Vocabulary
**Evidence:** VoxCore's INITIAL_PROMPT is static in config
**Root Cause:** No mechanism to update Whisper hints from learned terms
**Impact:** Whisper never improves on user-specific vocabulary

#### 5. Missing Confidence Data
**Evidence:** Confidence scores in JSON but not logged to tx_logs
**Root Cause:** Not tracked for analysis or quality assessment
**Impact:** Can't identify uncertain transcriptions for user review

---

## Architecture Principles

### VoxCore (Stateless)
- ✅ Reads vocabulary hints from **external files** (generated by VoxCompose)
- ✅ Captures **rich metadata** (confidence, app context, audio quality)
- ✅ Uses **standard INITIAL_PROMPT** from config + dynamic vocabulary
- ❌ Does NOT store learned corrections
- ❌ Does NOT maintain user profiles

### VoxCompose (Stateful)
- ✅ Learns from **user corrections** (new feedback mechanism)
- ✅ Generates **vocabulary files** for VoxCore's INITIAL_PROMPT
- ✅ Maintains **user profile** with learned patterns
- ✅ Tracks **correction effectiveness** over time
- ✅ Builds **domain-specific vocabularies** (coding, email, chat)

### Data Flow
```
User speaks → VoxCore captures audio
           ↓
VoxCore loads vocabulary hints from VoxCompose-generated file
           ↓
Whisper transcribes with improved INITIAL_PROMPT
           ↓
VoxCore applies stateless corrections (dictionary-based)
           ↓
[Optional] VoxCompose refines (LLM-based, for long recordings)
           ↓
Text pasted to app
           ↓
[New] User edits text → feedback captured
           ↓
VoxCompose learns from corrections → updates vocabulary file
           ↓
Next recording uses improved vocabulary
```

---

## Proposed Improvements

## Phase 1: Data Enrichment (Foundation)

**Goal:** Capture the data needed to measure and improve accuracy.

### 1.1 VoxCore: Enhanced Metadata Capture

**What to add to tx_logs JSONL:**

```json
{
  // Existing fields...
  "tx_ms": 464.738,

  // NEW: Whisper confidence metrics
  "confidence": {
    "mean": 0.89,              // Average across all segments
    "min": 0.72,               // Lowest confidence segment
    "max": 0.98,               // Highest confidence segment
    "low_confidence_segments": [  // Segments below threshold
      {"text": "pushto", "confidence": 0.58, "offset_ms": 1200}
    ]
  },

  // NEW: Application context
  "app_context": {
    "bundle_id": "com.apple.Notes",
    "app_name": "Notes",
    "window_title": "Untitled",
    "detected_mode": "prose"    // prose|code|chat|email (heuristic)
  },

  // NEW: Audio quality metrics
  "audio_quality": {
    "sample_rate": 16000,
    "duration_sec": 4.79,
    "rms_amplitude": 0.42,      // Root mean square
    "peak_amplitude": 0.87,
    "clipping_detected": false,
    "silence_ratio": 0.12       // % of audio below threshold
  },

  // NEW: Initial prompt metadata
  "initial_prompt": {
    "source": "voxcompose",     // config|voxcompose|none
    "word_count": 42,
    "file_path": "~/.config/voxcompose/vocabulary.txt",
    "file_mtime": "2025-12-04T00:00:00Z"
  },

  // NEW: Post-processing stats
  "post_processing": {
    "disfluency_removals": 2,   // "um", "uh" stripped
    "concatenation_fixes": 1,   // "pushto" → "push to"
    "dictionary_corrections": 3 // Custom dictionary fixes
  }
}
```

**Implementation:**
- Modify `whisper_wrapper.lua` to parse Whisper JSON segments for confidence
- Add `hs.application.frontmostApplication()` to capture app context
- Use ffmpeg stats to calculate audio quality metrics
- Track INITIAL_PROMPT source and contents

**Files to modify:**
- `/Users/cliffmin/code/voxcore/hammerspoon/whisper_wrapper.lua` (lines ~200-250)
- `/Users/cliffmin/code/voxcore/hammerspoon/push_to_talk.lua` (lines ~1860-1900)

### 1.2 VoxCore: Segment-Level Storage

**Create new file format:** `{timestamp}.segments.jsonl`

```jsonl
{"segment":1,"text":" All right,","from_ms":0,"to_ms":800,"confidence":0.92,"speaker_energy":0.45}
{"segment":2,"text":" let's see.","from_ms":800,"to_ms":1600,"confidence":0.88,"speaker_energy":0.52}
{"segment":3,"text":" Yeah, that's pretty cool, actually.","from_ms":1600,"to_ms":3600,"confidence":0.91,"speaker_energy":0.67}
```

**Why:** Enables fine-grained analysis of which segments have accuracy issues.

**Implementation:**
- Parse Whisper JSON output in `whisper_wrapper.lua`
- Write JSONL file alongside `.json` and `.txt`
- Include confidence, timestamps, audio energy per segment

### 1.3 VoxCompose: Feedback Capture Mechanism

**Problem:** VoxCompose learns from LLM diffs, not actual user corrections.

**Solution:** Capture user edits post-paste.

**Approach A: Clipboard Monitoring (Non-invasive)**

```lua
-- In push_to_talk.lua after pasting
local function captureUserFeedback(originalText, recordingId)
  -- Monitor clipboard for changes in next 60 seconds
  local feedbackTimer = hs.timer.doAfter(60, function()
    local clipboardText = hs.pasteboard.getContents()
    if clipboardText and clipboardText ~= originalText then
      -- User edited the text
      local editDistance = levenshtein(originalText, clipboardText)
      if editDistance > 0 then
        -- Write feedback file
        local feedbackPath = string.format(
          "~/.local/state/macos-ptt-dictation/feedback/%s.jsonl",
          os.date("%Y-%m-%d")
        )
        writeFeedback(feedbackPath, {
          ts = os.date("!%Y-%m-%dT%H:%M:%SZ"),
          recording_id = recordingId,
          original = originalText,
          corrected = clipboardText,
          edit_distance = editDistance,
          edit_ratio = editDistance / #originalText
        })
      end
    end
  end)
end
```

**Approach B: Explicit Correction Command (More reliable)**

```lua
-- Add new hotkey: Cmd+Alt+Ctrl+Shift+C = "Correct Last Transcription"
hs.hotkey.bind({"cmd", "alt", "ctrl", "shift"}, "c", function()
  local correctedText = hs.pasteboard.getContents()
  if correctedText and lastTranscriptId then
    writeFeedback(feedbackPath, {
      ts = os.date("!%Y-%m-%dT%H:%M:%SZ"),
      recording_id = lastTranscriptId,
      original = lastTranscript,
      corrected = correctedText,
      user_initiated = true
    })
    hs.alert.show("✓ Correction captured")
  end
end)
```

**Recommended:** Implement both. Clipboard monitoring for passive learning, explicit command for intentional corrections.

**Feedback File Location:**
```
~/.local/state/macos-ptt-dictation/feedback/
├── feedback-2025-12-03.jsonl
├── feedback-2025-12-04.jsonl
└── ...
```

**VoxCompose Integration:**
- VoxCompose reads feedback files on startup
- Learns corrections with higher confidence than LLM diffs
- Applies user corrections before LLM refinement

---

## Phase 2: Vocabulary Generation (Core Improvement)

**Goal:** Enable VoxCompose to generate vocabulary hints for Whisper.

### 2.1 VoxCompose: Vocabulary File Generation

**New Feature:** Generate `vocabulary.txt` from learned corrections.

**File Format:** Simple text file compatible with Whisper's INITIAL_PROMPT
```
GitHub, JSON, API, React, TypeScript, Kubernetes, PostgreSQL
refactor the authentication module
fix the race condition
implement retry logic with exponential backoff
```

**Generation Logic:**

```java
// New class: VocabularyGenerator.java
public class VocabularyGenerator {

  public String generateVocabularyHints(UserProfile profile, int maxTokens) {
    List<String> hints = new ArrayList<>();

    // 1. Technical terms from corrections
    for (Map.Entry<String, String> correction : profile.getWordCorrections().entrySet()) {
      if (isTechnicalTerm(correction.getValue())) {
        hints.add(correction.getValue());
      }
    }

    // 2. Capitalization patterns (proper nouns)
    for (String term : profile.getCapitalizations().values()) {
      if (isProperNoun(term)) {
        hints.add(term);
      }
    }

    // 3. Technical vocabulary list
    hints.addAll(profile.getTechnicalVocabulary());

    // 4. Common phrases from feedback
    hints.addAll(extractCommonPhrases(profile.getFeedbackHistory()));

    // 5. Domain-specific terms (if app context available)
    hints.addAll(getDomainVocabulary(profile.getPrimaryAppContext()));

    // Prioritize by frequency and recency
    hints = rankByRelevance(hints);

    // Limit to maxTokens (Whisper has ~223 token limit for INITIAL_PROMPT)
    return truncateToTokenLimit(hints, maxTokens);
  }

  private List<String> extractCommonPhrases(List<Feedback> history) {
    // Find 2-5 word phrases that appear frequently in corrections
    Map<String, Integer> phraseFrequency = new HashMap<>();

    for (Feedback fb : history) {
      String[] words = fb.getCorrectedText().split("\\s+");
      for (int len = 2; len <= 5; len++) {
        for (int i = 0; i <= words.length - len; i++) {
          String phrase = String.join(" ", Arrays.copyOfRange(words, i, i + len));
          phraseFrequency.merge(phrase, 1, Integer::sum);
        }
      }
    }

    // Return phrases that appear 3+ times
    return phraseFrequency.entrySet().stream()
      .filter(e -> e.getValue() >= 3)
      .sorted(Map.Entry.<String, Integer>comparingByValue().reversed())
      .map(Map.Entry::getKey)
      .limit(20)
      .collect(Collectors.toList());
  }
}
```

**Output File:**
```
~/.config/voxcompose/vocabulary.txt
```

**Update Frequency:**
- Regenerate after every 10 new corrections
- Or every 24 hours (whichever comes first)
- Write with timestamp for VoxCore to detect freshness

### 2.2 VoxCore: Dynamic Vocabulary Loading

**Modify INITIAL_PROMPT generation in `whisper_wrapper.lua`:**

```lua
-- Current (static):
local INITIAL_PROMPT = "Um, uh, like, you know. GitHub, JSON, API."

-- Proposed (dynamic):
local function loadInitialPrompt()
  local vocabFile = os.getenv("HOME") .. "/.config/voxcompose/vocabulary.txt"
  local staticPrompt = "Um, uh, like, you know."  -- Keep disfluency hints

  -- Check if VoxCompose vocabulary exists
  local f = io.open(vocabFile, "r")
  if f then
    local dynamicVocab = f:read("*all")
    f:close()
    return staticPrompt .. " " .. dynamicVocab
  else
    -- Fallback to config-based vocabulary
    return staticPrompt .. " " .. (CONFIG.WHISPER_INITIAL_PROMPT or "")
  end
end

-- Load on each transcription (file is read, not cached - stateless)
local initialPrompt = loadInitialPrompt()
```

**Configuration:**
```lua
-- In ptt_config.lua
VOCABULARY = {
  ENABLE_VOXCOMPOSE_VOCAB = true,  -- Use vocabulary from VoxCompose
  VOCAB_FILE = "~/.config/voxcompose/vocabulary.txt",
  FALLBACK_VOCAB = "GitHub, JSON, API, React",  -- If file missing
  MAX_VOCAB_AGE_HOURS = 168  -- Warn if vocabulary is stale (7 days)
}
```

### 2.3 Integration Testing

**Validation workflow:**

1. VoxCompose learns correction: `github` → `GitHub`
2. VoxCompose regenerates `vocabulary.txt` with `GitHub` included
3. Next VoxCore recording loads updated vocabulary
4. Whisper transcribes with `GitHub` in INITIAL_PROMPT
5. Improved accuracy on proper noun capitalization

**Metrics to track:**
- Transcription accuracy before/after vocabulary update
- Number of vocabulary terms used per recording
- Edit distance improvement over time

---

## Phase 3: Domain-Specific Adaptation

**Goal:** Adapt transcription based on application context.

### 3.1 App-Specific Vocabularies

**Concept:** Different apps need different vocabularies.

**Example domains:**
- **Code (VS Code, Cursor):** `refactor`, `function`, `variable`, `debug`, `commit`, `merge`
- **Chat (Slack, Discord):** `hey`, `lol`, `btw`, `imo`, `thread`, `channel`
- **Email (Mail, Gmail):** `regarding`, `attached`, `please`, `thank you`, `best regards`
- **Docs (Notion, Google Docs):** `section`, `heading`, `bullet point`, `summary`

**VoxCompose Implementation:**

```java
// New class: DomainVocabularyManager.java
public class DomainVocabularyManager {
  private Map<String, List<String>> domainVocabularies;

  public DomainVocabularyManager() {
    this.domainVocabularies = new HashMap<>();
    loadDefaultDomains();
  }

  private void loadDefaultDomains() {
    domainVocabularies.put("code", Arrays.asList(
      "refactor", "function", "variable", "debug", "commit", "push", "merge",
      "API", "JSON", "SQL", "async", "await", "promise", "callback"
    ));

    domainVocabularies.put("chat", Arrays.asList(
      "hey", "btw", "imo", "lol", "thread", "channel", "ping", "mentioned"
    ));

    domainVocabularies.put("email", Arrays.asList(
      "regarding", "attached", "enclosed", "please find", "thank you",
      "best regards", "sincerely", "forwarding"
    ));

    domainVocabularies.put("prose", Arrays.asList(
      "however", "therefore", "moreover", "furthermore", "consequently"
    ));
  }

  public String detectDomain(String bundleId, String appName) {
    if (bundleId.contains("code") || bundleId.contains("cursor")) return "code";
    if (bundleId.contains("slack") || bundleId.contains("discord")) return "chat";
    if (bundleId.contains("mail") || bundleId.contains("gmail")) return "email";
    return "prose";  // Default
  }

  public List<String> getVocabularyForDomain(String domain) {
    return domainVocabularies.getOrDefault(domain, Collections.emptyList());
  }
}
```

**VoxCore Integration:**

```lua
-- In push_to_talk.lua
local function getAppDomain()
  local app = hs.application.frontmostApplication()
  if not app then return "prose" end

  local bundleId = app:bundleID() or ""

  -- Simple heuristics
  if bundleId:match("code") or bundleId:match("cursor") then
    return "code"
  elseif bundleId:match("slack") or bundleId:match("discord") then
    return "chat"
  elseif bundleId:match("mail") or bundleId:match("gmail") then
    return "email"
  else
    return "prose"
  end
end

-- Pass domain to VoxCompose
local domain = getAppDomain()
local refinerCmd = string.format(
  "voxcompose --domain %s --duration %.2f",
  domain,
  duration
)
```

**VoxCompose vocabulary generation:**
```java
// In VocabularyGenerator.java
public String generateVocabularyHints(UserProfile profile, String domain, int maxTokens) {
  List<String> hints = new ArrayList<>();

  // 1. Domain-specific vocabulary (high priority)
  hints.addAll(domainManager.getVocabularyForDomain(domain));

  // 2. User's learned corrections
  hints.addAll(getUserCorrections(profile));

  // 3. General technical terms
  hints.addAll(profile.getTechnicalVocabulary());

  // Rank and truncate
  return rankAndTruncate(hints, maxTokens);
}
```

### 3.2 Learning Domain Patterns

**Goal:** Automatically learn which apps use which vocabularies.

**VoxCompose tracking:**

```java
// In UserProfile.java
private Map<String, DomainStatistics> domainStats;

class DomainStatistics {
  private String domain;
  private int usageCount;
  private Set<String> frequentTerms;
  private double avgDuration;
  private LocalDateTime lastUsed;
}

// Learn from tx_logs
public void updateDomainStats(String domain, String transcript) {
  DomainStatistics stats = domainStats.computeIfAbsent(
    domain,
    k -> new DomainStatistics(k)
  );

  stats.incrementUsage();
  stats.extractTerms(transcript);  // Add to frequent terms
  stats.updateLastUsed();
}
```

**Result:** Over time, VoxCompose learns:
- Which apps you use most
- Which technical terms appear in which apps
- Domain-specific corrections to prioritize

---

## Phase 4: Quality Assurance & Feedback

**Goal:** Measure improvements and iterate.

### 4.1 Accuracy Metrics Dashboard

**New tool:** `voxcore-stats` command

```bash
# Show accuracy metrics
voxcore-stats --accuracy

# Output:
Transcription Accuracy (Last 30 Days)
=====================================
Total recordings:        347
With user corrections:   42 (12.1%)
Avg edit distance:       3.2 characters
Improvement over time:   +8.4% (vs 30 days ago)

Top correction patterns:
  "github" → "GitHub"     (14 times)
  "pushto" → "push to"    (8 times)
  "api" → "API"          (7 times)

Domain breakdown:
  Code:   172 recordings, 15.3% correction rate
  Prose:  98 recordings,  8.1% correction rate
  Chat:   77 recordings,  10.4% correction rate
```

**Implementation:**

```bash
#!/usr/bin/env bash
# voxcore-stats script

FEEDBACK_DIR="$HOME/.local/state/macos-ptt-dictation/feedback"
TX_LOGS_DIR="$HOME/.local/state/macos-ptt-dictation/tx_logs"

# Parse feedback files and tx_logs
python3 <<EOF
import json
import glob
from datetime import datetime, timedelta

# Load all feedback
feedback = []
for file in glob.glob("$FEEDBACK_DIR/*.jsonl"):
    with open(file) as f:
        feedback.extend([json.loads(line) for line in f])

# Load all transactions
transactions = []
for file in glob.glob("$TX_LOGS_DIR/*.jsonl"):
    with open(file) as f:
        transactions.extend([json.loads(line) for line in f])

# Calculate metrics
total_recordings = len(transactions)
with_corrections = len(feedback)
correction_rate = with_corrections / total_recordings * 100

print(f"Total recordings: {total_recordings}")
print(f"With corrections: {with_corrections} ({correction_rate:.1f}%)")
# ... more analysis
EOF
```

### 4.2 Confidence-Based Review

**Feature:** Flag low-confidence transcriptions for user review.

**VoxCore notification:**

```lua
-- After transcription completes
local function checkConfidence(whisperJson)
  local segments = whisperJson.transcription or {}
  local lowConfSegments = {}

  for _, seg in ipairs(segments) do
    if seg.confidence and seg.confidence < 0.75 then
      table.insert(lowConfSegments, seg.text)
    end
  end

  if #lowConfSegments > 0 then
    -- Show notification
    hs.notify.new({
      title = "VoxCore",
      informativeText = "Low confidence detected. Review transcription?",
      actionButtonTitle = "Review",
      otherButtonTitle = "Ignore"
    }, function(notification)
      if notification:activationType() == hs.notify.activationTypes.actionButtonClicked then
        -- Open review UI (could be simple text window)
        showReviewWindow(lastTranscript, lowConfSegments)
      end
    end):send()
  end
end
```

### 4.3 A/B Testing Framework

**Goal:** Compare different vocabularies/models systematically.

**VoxCore experiment tracking:**

```lua
-- In ptt_config.lua
EXPERIMENTS = {
  ENABLE_AB_TESTING = true,
  CURRENT_EXPERIMENT = "vocab_v2",
  EXPERIMENTS = {
    {
      id = "vocab_v2",
      description = "Test VoxCompose-generated vocabulary",
      start_date = "2025-12-04",
      variants = {
        { name = "control", weight = 0.5, vocab_source = "config" },
        { name = "voxcompose", weight = 0.5, vocab_source = "voxcompose" }
      }
    }
  }
}
```

**Random assignment:**

```lua
-- In whisper_wrapper.lua
local function getExperimentVariant(userId)
  local exp = CONFIG.EXPERIMENTS.CURRENT_EXPERIMENT
  if not exp then return nil end

  -- Consistent hash-based assignment
  local hash = hashString(userId .. exp.id)
  local roll = hash % 100

  local cumulative = 0
  for _, variant in ipairs(exp.variants) do
    cumulative = cumulative + (variant.weight * 100)
    if roll < cumulative then
      return variant.name
    end
  end
end

-- Use variant to determine vocabulary source
local variant = getExperimentVariant(machineId)
if variant == "voxcompose" then
  initialPrompt = loadVoxComposeVocabulary()
else
  initialPrompt = loadConfigVocabulary()
end

-- Log experiment assignment
logTransaction({
  experiment_id = exp.id,
  variant = variant,
  -- ... other fields
})
```

**Analysis:**

```bash
voxcore-stats --experiment vocab_v2

# Output:
Experiment: vocab_v2 (7 days active)
=====================================
Control group:     173 recordings, 12.7% correction rate
VoxCompose group:  174 recordings,  9.1% correction rate

Result: VoxCompose vocabulary shows 28% reduction in corrections (p<0.05)
Recommendation: Roll out to 100% of users
```

---

## Phase 5: Advanced Features (Future)

### 5.1 Context-Aware Capitalization

**Problem:** Whisper lowercases most proper nouns.

**Solution:** Use app context + learned patterns.

**Example:**
- In code editor: `react`, `github`, `json` → `React`, `GitHub`, `JSON`
- In prose: `john` → `John` (proper noun detection)
- In chat: `lol` → `lol` (keep lowercase)

**VoxCompose implementation:**

```java
// CapitalizationEngine.java
public String applyContextualCapitalization(String text, String domain) {
  if ("code".equals(domain)) {
    // Apply technical term capitalizations
    text = applyPattern(text, technicalTermPatterns);
  } else if ("prose".equals(domain)) {
    // Apply proper noun detection
    text = applyProperNounCapitalization(text);
  }
  // Chat keeps most things lowercase
  return text;
}
```

### 5.2 Multi-Pass Transcription

**Concept:** For critical recordings, transcribe twice with different models.

**Workflow:**
1. User holds hotkey longer (>10s) to indicate "important"
2. VoxCore transcribes with both `base.en` and `small.en`
3. Compare outputs, flag disagreements
4. User reviews only the uncertain parts

**Implementation:**

```lua
-- In push_to_talk.lua
local function handleLongRecording(audioPath, duration)
  if duration > 10 and CONFIG.ENABLE_MULTI_PASS then
    hs.alert.show("High-accuracy mode")

    -- Transcribe with base.en
    local baseResult = transcribe(audioPath, "base.en")

    -- Transcribe with small.en
    local smallResult = transcribe(audioPath, "small.en")

    -- Compare
    local differences = compareTranscripts(baseResult, smallResult)

    if #differences > 0 then
      -- Show review UI
      showDiffReview(baseResult, smallResult, differences)
    else
      -- Both agree, use base.en (faster)
      return baseResult
    end
  end
end
```

### 5.3 Audio Quality Warnings

**Feature:** Alert user if audio quality is poor.

**Implementation:**

```lua
-- After recording stops
local function checkAudioQuality(wavPath)
  local stats = getAudioStats(wavPath)  -- Uses ffmpeg

  if stats.peak_amplitude < 0.1 then
    hs.alert.show("⚠️ Recording very quiet. Speak louder?")
  elseif stats.clipping_detected then
    hs.alert.show("⚠️ Audio clipping detected. Move mic further?")
  elseif stats.silence_ratio > 0.5 then
    hs.alert.show("⚠️ Mostly silence detected. Check microphone?")
  end
end
```

### 5.4 Collaborative Learning

**Concept:** Opt-in sharing of anonymized corrections.

**Privacy-preserving approach:**
1. User opts in to "Improve VoxCore for everyone"
2. VoxCompose uploads only correction patterns (not audio/text)
3. Central service aggregates common corrections
4. Users download community vocabulary updates

**Example shared data:**
```json
{
  "pattern": "github",
  "correction": "GitHub",
  "frequency": 847,
  "domains": ["code"],
  "confidence": 0.96
}
```

**Not shared:** Audio files, full transcripts, personal vocabulary.

---

## Implementation Roadmap

### Phase 1: Fix Broken Behavior + Hardening (1-2 weeks)

**Goal:** Fix wrong behavior if it exists, no new features. Ensure current system works as intended.

**CRITICAL:** README promises "learns your speech patterns" but VoxCompose learning is NOT connected to VoxCore. This is BROKEN and must be fixed.

#### 1.1 Fix Broken Vocabulary Integration ⚠️ **CRITICAL**

**Problem:** VoxCompose learns vocabulary (`technicalVocabulary` list exists) but doesn't export it. VoxCore can't use learned terms. The learning is isolated and doesn't improve future transcriptions.

**Current broken flow:**
```
VoxCompose learns terms → stores in UserProfile → ❌ NOT exported
VoxCore transcribes → uses static INITIAL_PROMPT → ❌ never improves
```

**Fixed flow (Phase 1):**
```
VoxCompose learns terms → exports to vocabulary.txt → VoxCore loads dynamically → better transcriptions
```

- [ ] **VoxCompose: Export vocabulary command**
  - Add `--export-vocabulary` CLI flag
  - Write `UserProfile.technicalVocabulary` to `~/.config/voxcompose/vocabulary.txt`
  - Format: Comma-separated terms (compatible with INITIAL_PROMPT)
  - Auto-export on profile save (after learning)
  - Limit to ~1000 words (Whisper token limit)

- [ ] **VoxCore: Load vocabulary dynamically** (move from Phase 2)
  - Implement `loadInitialPrompt()` in `whisper_wrapper.lua`
  - Read from `~/.config/voxcompose/vocabulary.txt` if exists
  - Append to static prompt: `"Um, uh. " + vocabulary`
  - Graceful fallback if file missing
  - Performance: ~1ms read (negligible overhead)

- [ ] **Integration testing**
  - Test: VoxCompose learns → exports → VoxCore loads → improves accuracy
  - Verify learned terms actually used by Whisper
  - Test fallback when VoxCompose not installed
  - Measure accuracy improvement with learned vocabulary

**Why Phase 1, not Phase 2:**
- Infrastructure already exists (VoxCompose has `technicalVocabulary`)
- README claims this works ("learns your speech patterns")
- This is BROKEN behavior, not a new feature
- Simple fix: connect existing components

#### 1.2 Configuration System Improvements

**Problem:** Hotkeys, storage paths, and env vars not fully configurable or tested.

- [ ] **Hotkey configuration testing**
  - Test all hotkey combinations work (Cmd, Alt, Ctrl, Shift)
  - Validate hotkey config on load (catch conflicts early)
  - Document hotkey options in config

- [ ] **Storage directory configuration**
  - Make `VOICE_NOTES_DIR` configurable in `ptt_config.lua`
  - Default: `~/Documents/VoiceNotes`
  - Create directory if doesn't exist
  - Validate path is writable on startup
  - Support `~` expansion and env vars

- [ ] **TX logs directory configuration**
  - Make `TX_LOGS_DIR` configurable
  - Default: `~/.local/state/macos-ptt-dictation/tx_logs`
  - Support custom paths via config

- [ ] **Environment variable validation & testing**
  - `OLLAMA_HOST` - VoxCompose LLM endpoint (validate URL)
  - `PTT_CONFIG_FILE` - custom config path (validate exists)
  - `WHISPER_CPP_PATH` - custom Whisper binary (validate executable)
  - `VOXCOMPOSE_BIN` - custom VoxCompose path (validate executable)
  - Test all env vars work correctly
  - Document all env vars in README
  - Provide good error messages if invalid

- [ ] **Config validation & defaults**
  - Validate config on load (catch errors early)
  - Sensible defaults for all options
  - Warn about deprecated/unknown keys
  - Config migration for breaking changes

####1.3 Code Audit & Bug Fixes

- [ ] Audit existing Whisper integration for correctness
- [ ] Fix any silent failures in transcription pipeline
- [ ] Ensure WAV files are always saved (reliability promise)
- [ ] Fix error handling in VoxCompose integration
- [ ] Validate JSON output parsing from Whisper
- [ ] Check for memory leaks (Lua timers/callbacks)
- [ ] Verify symlink handling works correctly

#### 1.4 Testing Infrastructure

- [ ] **Unit tests** for critical paths:
  - Audio file writing (WAV always saved)
  - Config loading and validation
  - Vocabulary file loading
  - Error handling paths
  - JSON parsing

- [ ] **Integration tests:**
  - VoxCore → VoxCompose flow (with/without VoxCompose)
  - Vocabulary export → load → transcription
  - Config validation (valid/invalid configs)
  - Hotkey bindings

- [ ] **Edge case tests:**
  - Empty audio recording
  - Very long recordings (>5 min)
  - Special characters in transcripts
  - Missing/corrupted config files
  - Disk full scenarios
  - Concurrent recordings

- [ ] **Verify tx_logs accuracy:**
  - Timestamps match actual recording time
  - Durations correct
  - Model names logged correctly
  - Config values logged
  - Vocabulary source tracked

#### 1.5 Error Recovery & UX

- [ ] Graceful degradation if VoxCompose unavailable
- [ ] Retry logic for ffmpeg failures (3 attempts with backoff)
- [ ] Better error messages (actionable, user-friendly):
  - "Microphone permission denied" → link to System Settings
  - "Whisper model not found" → suggest installation command
  - "Disk full" → show available space
  - "Config invalid" → show which field and why
- [ ] Log errors without crashing Hammerspoon
- [ ] User-facing error notifications (not just console logs)
- [ ] Error telemetry to tx_logs (for debugging)

**Deliverable:**
- ✅ Broken vocabulary integration FIXED (learning now improves transcriptions)
- ✅ Config system fully configurable and tested
- ✅ All env vars validated and documented
- ✅ Stable, tested, reliable foundation
- ✅ No regressions

---

### Phase 2: Quick Wins & Feature Augmentation (2-3 weeks)

**Goal:** Improve accuracy with minimal risk. Augment existing features without breaking changes.

**Note:** Basic vocabulary loading/export is in Phase 1 (fixing broken behavior). Phase 2 enhances it with smarter generation and domain awareness.

#### 2.1 Enhanced Vocabulary Generation (VoxCompose)

**Prerequisite:** Phase 1 basic vocabulary export must be complete.

- [ ] Create `VocabularyGenerator.java` class (smarter than Phase 1)
- [ ] Include learned corrections (word-level fixes)
- [ ] Include capitalizations (GitHub, JSON, API)
- [ ] Extract common phrases from feedback (2-5 word patterns)
- [ ] Prioritize by frequency and recency (most common first)
- [ ] Rank by relevance score (frequency × recency × confidence)
- [ ] Limit to ~223 Whisper tokens (~1000 words max)
- [ ] Regenerate after every 10 corrections or daily (whichever first)

**Why Phase 2, not Phase 1:**
- Phase 1 just exports existing `technicalVocabulary` list (simple fix)
- Phase 2 adds intelligence (phrase extraction, ranking, prioritization)

#### 2.3 Enhanced Metadata Capture (VoxCore)
- [ ] Add per-recording `.meta.json` files
- [ ] Capture Whisper confidence scores (mean, min, max)
- [ ] Capture app context (bundle ID, app name, detected domain)
- [ ] Capture audio quality metrics (RMS, peak amplitude, clipping detection)
- [ ] Track INITIAL_PROMPT source (config | voxcompose | none)
- [ ] Store vocabulary file metadata (path, mtime, word count)

**Metadata file structure:** (see detailed spec in plan)

#### 2.4 Domain Detection (VoxCore)
**Status:** Designed (see Q&A section below)

- [ ] Implement `detectDomain()` using bundle ID pattern matching
- [ ] Simple heuristics: code, chat, email, terminal, prose
- [ ] NOT LLM-based (too slow)
- [ ] Pass domain to VoxCompose via `--domain` flag
- [ ] Log domain in tx_logs for analysis

**Domains:**
- `code`: VS Code, Cursor, Xcode, Android Studio
- `chat`: Slack, Discord, Telegram, Messages
- `email`: Mail, Gmail, Outlook, Spark
- `terminal`: Terminal, iTerm
- `prose`: Default fallback

#### 2.5 Domain-Specific Vocabularies (VoxCompose)
- [ ] Generate separate vocabulary files per domain
  - `vocabulary-code.txt` - technical programming terms
  - `vocabulary-chat.txt` - casual language
  - `vocabulary-email.txt` - professional language
  - `vocabulary-terminal.txt` - command-line terms
  - `vocabulary-prose.txt` - general writing
- [ ] VoxCore loads domain-specific file based on detected domain
- [ ] Fallback to generic `vocabulary.txt` if domain file missing
- [ ] Default domain vocabularies (hardcoded base sets)

#### 2.6 Integration Testing
- [ ] Test VoxCore → VoxCompose vocabulary flow end-to-end
- [ ] Verify vocabulary updates propagate to next transcription
- [ ] Test domain detection accuracy (sample 100 apps)
- [ ] Measure accuracy improvement (baseline vs. with vocabulary)

**Deliverable:** Bidirectional vocabulary sharing + domain awareness

---

### Phase 3: Advanced Features & New Behaviors (3-4 weeks)

**Goal:** Big new capabilities that change user workflows.

#### 3.1 Manual Correction Feature
**Status:** Designed - see [FUTURE_MANUAL_CORRECTION_FEATURE.md](./FUTURE_MANUAL_CORRECTION_FEATURE.md)

- [ ] Hotkey for explicit corrections (`Cmd+Alt+Ctrl+Shift+C`)
- [ ] Capture corrected text from clipboard
- [ ] Calculate detailed diff (word-level changes)
- [ ] Save correction to `.meta.json`
- [ ] VoxCompose learns from user corrections (high confidence)
- [ ] Correction statistics UI

**Note:** This is a major UX change. Requires user education and testing.

#### 3.2 Passive Clipboard Monitoring (Optional)
- [ ] Monitor clipboard for 60s after paste
- [ ] Detect if user edited the pasted text
- [ ] Calculate edit distance
- [ ] Write to feedback file if changed
- [ ] Lower confidence than explicit corrections

**Risk:** Privacy concerns, clipboard conflicts. Make opt-in.

#### 3.3 A/B Testing Framework
- [ ] Experiment configuration in `ptt_config.lua`
- [ ] Random variant assignment (hash-based, consistent)
- [ ] Log experiment ID and variant in tx_logs
- [ ] Analysis tool: `voxcore-stats --experiment {id}`
- [ ] Statistical significance testing
- [ ] Automatic rollout based on results

**Use cases:**
- Test vocabulary v1 vs v2
- Test different Whisper models
- Test preprocessing configurations

#### 3.4 Accuracy Analytics
- [ ] `voxcore-stats` command-line tool
- [ ] Parse tx_logs and metadata files
- [ ] Calculate accuracy metrics (correction rate, edit distance)
- [ ] Domain breakdown (code vs chat vs email)
- [ ] Trend analysis (improvement over time)
- [ ] Correction pattern analysis (most common fixes)

#### 3.5 Confidence-Based Review
- [ ] Detect low-confidence transcriptions (<0.75 mean)
- [ ] Show notification asking user to review
- [ ] Optional: Show review UI with highlighted uncertain segments
- [ ] User can mark as "correct" or provide correction
- [ ] Feed back into learning system

#### 3.6 Audio Quality Warnings
- [ ] Analyze audio after recording
- [ ] Warn if too quiet (peak amplitude <0.1)
- [ ] Warn if clipping detected
- [ ] Warn if mostly silence (>50% below threshold)
- [ ] Help users improve audio quality

**Deliverable:** Complete feedback loop, user-driven learning, quality tooling

---

### Future: Advanced Features (Not Scheduled)

See separate documents:
- [Manual Correction Feature](./FUTURE_MANUAL_CORRECTION_FEATURE.md) - Explicit user corrections via hotkey
- Context-aware capitalization (domain-specific casing rules)
- Multi-pass transcription (use multiple models for critical recordings)
- Collaborative learning (opt-in community vocabulary sharing)

---

## Success Metrics

### Primary KPIs
1. **User correction rate:** % of transcriptions that get edited
   - **Current baseline:** Unknown (not tracked)
   - **Target:** <5% correction rate for typical users

2. **Edit distance:** Average characters changed per transcription
   - **Current baseline:** Unknown
   - **Target:** <2 characters per 100 characters transcribed

3. **Whisper confidence:** Average confidence score
   - **Current baseline:** Unknown
   - **Target:** >0.90 mean confidence

4. **Domain accuracy:** Correction rate by application
   - **Target:** Code: <8%, Prose: <5%, Chat: <7%

### Secondary Metrics
- Vocabulary file freshness (days since update)
- VoxCompose learning rate (corrections/week)
- User engagement with feedback mechanism (% using explicit correction command)
- A/B test statistical power (sample size for significance)

---

## Data Requirements

### Storage Estimates

**Current:**
- Per recording: ~150KB WAV + 5KB JSON + 1KB txt = 156KB
- 100 recordings/day = 15.6MB/day = 5.7GB/year

**With improvements:**
- Per recording: 156KB + 2KB segments.jsonl + 1KB feedback = 159KB
- 100 recordings/day = 15.9MB/day = 5.8GB/year

**Increase:** +100MB/year (negligible)

### Privacy Considerations

**What's stored locally:**
- ✅ Audio recordings (already stored)
- ✅ Full transcripts (already stored)
- ✅ User corrections (new, but opt-in)
- ✅ App bundle IDs (for domain detection)
- ✅ Confidence scores (metadata only)

**What's NOT stored:**
- ❌ Window titles or document contents
- ❌ Keystrokes or other app data
- ❌ Any network transmission (unless collaborative learning is explicitly enabled)

**User control:**
- All data remains on local device
- Users can delete recordings anytime
- Opt-in for clipboard monitoring
- Opt-in for collaborative learning (Phase 5)

---

## Risk Assessment

### Technical Risks

1. **Vocabulary file conflicts**
   - **Risk:** VoxCore and VoxCompose both trying to write vocabulary.txt
   - **Mitigation:** VoxCompose owns the file, VoxCore only reads (stateless)

2. **INITIAL_PROMPT token limit**
   - **Risk:** Exceeding Whisper's ~223 token limit
   - **Mitigation:** VocabularyGenerator truncates intelligently

3. **Stale vocabulary**
   - **Risk:** Vocabulary file not updated, VoxCore uses outdated hints
   - **Mitigation:** Check file mtime, warn if >7 days old

4. **Performance degradation**
   - **Risk:** Reading vocabulary file adds latency
   - **Mitigation:** File is small (<5KB), read is <1ms

### User Experience Risks

1. **Feedback fatigue**
   - **Risk:** Users annoyed by correction requests
   - **Mitigation:** Passive clipboard monitoring (non-intrusive) + optional explicit command

2. **Privacy concerns**
   - **Risk:** Users worried about data collection
   - **Mitigation:** Clear documentation, all data local, opt-in for any sharing

3. **Accuracy regression**
   - **Risk:** Bad vocabulary updates make things worse
   - **Mitigation:** A/B testing, rollback capability, user can disable

---

## Design Q&A - Clarifications from Planning Discussion

### Q1: Dynamic Vocabulary Loading - Performance & Caching?

**Question:** Should VoxCore read the vocabulary file on every transcription, or cache it? What about performance?

**Answer:** **Read on every transcription** (no caching).

**Reasoning:**
- Small file size (~5KB max, ~200-500 words)
- File I/O is fast: **~1ms** on modern SSDs
- Keeps VoxCore truly stateless (no stale data)
- VoxCompose can update file between transcriptions → next recording gets fresh vocab
- Performance overhead: **<0.2%** (1ms out of ~500-1000ms total transcription time)

**Implementation:**
```lua
local function loadInitialPrompt()
  local vocabFile = os.getenv("HOME") .. "/.config/voxcompose/vocabulary.txt"
  local f = io.open(vocabFile, "r")

  if f then
    local dynamicVocab = f:read("*all")
    f:close()
    return staticPrompt .. " " .. dynamicVocab
  else
    -- Fallback to config-based vocabulary
    return staticPrompt .. " " .. CONFIG.WHISPER_INITIAL_PROMPT
  end
end

-- Call on EVERY transcription (not cached)
local initialPrompt = loadInitialPrompt()
```

**Configuration:**
- Optional via `ENABLE_DYNAMIC_VOCAB` flag
- Graceful fallback if file doesn't exist
- No breaking changes for existing users

---

### Q2: User Feedback - Metadata Files vs Clipboard Monitoring?

**Question:** Should we use clipboard monitoring or a better approach for feedback capture?

**Answer:** **Use per-recording metadata files + multiple feedback methods**.

**Better approach than clipboard-only:**

Create `.meta.json` file for each recording:
```
~/Documents/VoiceNotes/2025-Dec-04_12.30.00_PM/
├── 2025-Dec-04_12.30.00_PM.wav
├── 2025-Dec-04_12.30.00_PM.json
├── 2025-Dec-04_12.30.00_PM.txt
└── 2025-Dec-04_12.30.00_PM.meta.json  ← NEW
```

**Metadata file structure:**
```json
{
  "recording_id": "2025-Dec-04_12.30.00_PM",
  "transcript": {
    "original": "add error handling",
    "corrected": "Add error handling to DB module",
    "correction_method": "explicit_hotkey"
  },
  "context": {
    "app_bundle_id": "com.apple.Notes",
    "domain": "prose"
  },
  "quality": {
    "whisper_confidence_mean": 0.89,
    "audio_rms": 0.42
  },
  "user_feedback": {
    "marked_correct": false,
    "needs_review": false
  }
}
```

**Multiple feedback methods:**
1. **Clipboard monitoring** (passive, automatic)
2. **Explicit hotkey** (user-initiated: `Cmd+Alt+Ctrl+Shift+C`)
3. **Manual metadata editing** (power users)
4. **Review UI** (future - for low-confidence transcriptions)

**Benefits:**
- ✅ Future-proof - can add more metadata fields
- ✅ Helps with other use cases (A/B testing, quality tracking)
- ✅ Explicit user control
- ✅ VoxCompose learns from metadata files

**Note:** Manual correction feature moved to Phase 3 and detailed in [FUTURE_MANUAL_CORRECTION_FEATURE.md](./FUTURE_MANUAL_CORRECTION_FEATURE.md).

---

### Q3: Domain Detection - How is Domain Identified?

**Question:** How is domain detected? Via LLM? How does VoxCore use domains?

**Answer:** **Simple heuristics (bundle ID pattern matching), NOT LLM. Domains are used as an index to select vocabulary files.**

**Domain detection logic:**
```lua
local function detectDomain()
  local app = hs.application.frontmostApplication()
  if not app then return "prose" end

  local bundleId = app:bundleID() or ""

  -- Simple pattern matching (fast, no LLM)
  if bundleId:match("code") or bundleId:match("cursor") or
     bundleId:match("xcode") then
    return "code"

  elseif bundleId:match("slack") or bundleId:match("discord") then
    return "chat"

  elseif bundleId:match("mail") or bundleId:match("gmail") then
    return "email"

  else
    return "prose"  -- Default
  end
end
```

**How domains are used:**

VoxCompose generates **separate vocabulary files per domain**:
```
~/.config/voxcompose/
├── vocabulary-code.txt      # refactor, function, GitHub, API...
├── vocabulary-chat.txt      # hey, lol, btw, thread...
├── vocabulary-email.txt     # regarding, attached, sincerely...
└── vocabulary-prose.txt     # however, moreover, furthermore...
```

VoxCore loads the appropriate file:
```lua
local domain = detectDomain()
local vocabFile = string.format(
  "%s/.config/voxcompose/vocabulary-%s.txt",
  os.getenv("HOME"),
  domain
)
```

**Benefits:**
- Different vocabularies for different contexts
- Code: `refactor` more likely than `rephrase`
- Chat: `lol` more likely than `laugh out loud`
- Email: `regarding` more likely than `about`
- Smaller, focused vocabularies = better Whisper hints

**NOT LLM-based because:**
- Too slow (adds latency to every transcription)
- Unnecessary - bundle ID patterns work well
- Keeps VoxCore simple and stateless

---

### Q4: Manual Correction Feature - Plugin or Core?

**Question:** Should manual correction be a new plugin or part of VoxCore/VoxCompose?

**Answer:** **VoxCore feature (hotkey + metadata capture) + VoxCompose integration (learning).**

**Architecture:**
- **VoxCore (stateless):**
  - Handles correction hotkey (`Cmd+Alt+Ctrl+Shift+C`)
  - Captures corrected text from clipboard
  - Calculates diff (Levenshtein distance + word-level changes)
  - Writes correction to `.meta.json` file
  - Shows user feedback ("✓ Correction saved")

- **VoxCompose (stateful):**
  - Scans `.meta.json` files for corrections
  - Learns from user corrections (higher confidence than LLM diffs)
  - Updates UserProfile with learned patterns
  - Regenerates vocabulary files with new terms

**Why not a plugin?**
- Hotkey handling requires Hammerspoon (VoxCore's domain)
- Metadata file writing is already VoxCore's responsibility
- VoxCompose already reads metadata files (existing pattern)
- No new components needed

**Benefits:**
- Clean separation of concerns
- Works even if VoxCompose not installed (corrections stored for future)
- Reuses existing metadata infrastructure

**Status:** Detailed design in [FUTURE_MANUAL_CORRECTION_FEATURE.md](./FUTURE_MANUAL_CORRECTION_FEATURE.md). Implementation in Phase 3.

---

## Open Questions

1. **How to handle multi-language users?**
   - Current: English-only focus
   - Future: Per-language vocabularies?

2. **Should we support custom model fine-tuning?**
   - User provides corrected transcripts → fine-tune personal Whisper model
   - Requires significant compute + storage

3. **How to handle domain ambiguity?**
   - User switches between code and prose in same app (e.g., Notion with code blocks)
   - Possible solution: Hotkey to override domain detection

4. **Vocabulary size limits?**
   - How many terms can we realistically include in INITIAL_PROMPT?
   - Need empirical testing to find optimal size

5. **Correction confidence thresholds?**
   - When is a user edit a "real correction" vs. stylistic change?
   - May need edit distance thresholds or semantic similarity checks

---

## Conclusion

**Core Strategy:**
1. **VoxCore captures richer metadata** (confidence, app context, audio quality)
2. **VoxCompose generates vocabulary hints** from learned corrections
3. **VoxCore loads vocabulary dynamically** (remains stateless)
4. **User feedback closes the loop** (learn from real corrections, not just LLM diffs)
5. **Domain adaptation improves relevance** (different vocabularies for different apps)

**Expected Impact:**
- **20-40% reduction** in user correction rate
- **Improved proper noun recognition** (GitHub, JSON, API)
- **Better technical term handling** (refactor, async, PostgreSQL)
- **Context-appropriate transcription** (code vs. prose vs. chat)

**Next Steps:**
1. Review this plan with stakeholders
2. Prioritize Phase 1 (foundation) for immediate implementation
3. Set up accuracy baseline measurement
4. Begin enhanced metadata capture
5. Iterate based on real-world data

---

**Document Status:** DRAFT - Ready for Review
**Author:** Claude Code
**Date:** 2025-12-04
**Version:** 1.0
